{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4VMspRYhaIu"
      },
      "outputs": [],
      "source": [
        "# !pip install accelerate -U\n",
        "# !pip install datasets\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "from transformers import ViTForImageClassification, Trainer, TrainingArguments, ViTFeatureExtractor\n",
        "from datasets import load_metric\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# pytorch imports\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, dataset, random_split\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 211\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)"
      ],
      "metadata": {
        "id": "bKfIYMhvhyrP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define basic transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "])"
      ],
      "metadata": {
        "id": "BOXJnsrMh2gW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = './drive/MyDrive/deep learn/project/dataset'   # change to your directory\n",
        "train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=transform)\n",
        "test_dataset = datasets.ImageFolder(os.path.join(data_dir, 'test'), transform=transform)\n",
        "# Split dataset into train and validation\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "labels = test_dataset.classes"
      ],
      "metadata": {
        "id": "TdGlS0LMh6vV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_pil = transforms.ToPILImage()\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "def convert_to_hf_format(dataset):\n",
        "    hf_dataset = {\"image\": [], \"labels\": []}\n",
        "    for idx in range(len(dataset)):\n",
        "        image, label = dataset[idx]\n",
        "        hf_dataset[\"image\"].append(to_pil(image))\n",
        "        hf_dataset[\"labels\"].append(label)\n",
        "\n",
        "    return hf_dataset"
      ],
      "metadata": {
        "id": "Cy3Cuq2qiD4H"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_train_dataset_dict = convert_to_hf_format(train_dataset)\n",
        "hf_val_dataset_dict = convert_to_hf_format(val_dataset)\n",
        "hf_test_dataset_dict = convert_to_hf_format(test_dataset)\n",
        "\n",
        "ds = DatasetDict({\"train\": Dataset.from_dict(hf_train_dataset_dict),\n",
        "                            \"validation\": Dataset.from_dict(hf_val_dataset_dict),\n",
        "                            \"test\": Dataset.from_dict(hf_test_dataset_dict)})"
      ],
      "metadata": {
        "id": "ta_5lapIiIdV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8TyOGN7iOHN",
        "outputId": "f752e202-3bd4-47d9-d24f-6e2b2525fd6f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(example_batch):\n",
        "    # Take a list of PIL images and turn them to pixel values\n",
        "    inputs = feature_extractor([x for x in example_batch['image']], return_tensors='pt')\n",
        "\n",
        "    # Don't forget to include the labels!\n",
        "    inputs['labels'] = example_batch['labels']\n",
        "    return inputs\n",
        "\n",
        "prepared_ds = ds.with_transform(transform)"
      ],
      "metadata": {
        "id": "Mn_uHf2LiVOt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "  # Converts pixel values to tensor\n",
        "    return {\n",
        "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
        "        'labels': torch.tensor([x['labels'] for x in batch])\n",
        "    }"
      ],
      "metadata": {
        "id": "AHHiNrgeiVnE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fH6NyVXfiVpt",
        "outputId": "682ef131-adde-422b-8ea3-33363de56a53"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-1ed244c6ebac>:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViTForImageClassification.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    num_labels=len(labels),\n",
        "    id2label={str(i): c for i, c in enumerate(labels)},\n",
        "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgddbwfCiVsf",
        "outputId": "8f41e65b-c06f-45d6-894c-64c5f5210c04"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./vit-base-elephants-v5\",\n",
        "    per_device_train_batch_size=16,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    num_train_epochs=3,\n",
        "    fp16=False,\n",
        "    save_steps=100,\n",
        "    eval_steps=100,\n",
        "    logging_steps=10,\n",
        "    learning_rate=2e-4,\n",
        "    save_total_limit=2,\n",
        "    remove_unused_columns=False,\n",
        "    push_to_hub=False,\n",
        "    report_to='tensorboard',\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ],
      "metadata": {
        "id": "n-uBRTEjiVv4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=prepared_ds[\"train\"],\n",
        "    eval_dataset=prepared_ds[\"validation\"],\n",
        "    tokenizer=feature_extractor,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTndOEe2irOO",
        "outputId": "e2776687-a155-462e-a28e-db125e87dfc3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_results = trainer.train()\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "jrtcjoYFirQv",
        "outputId": "c764dc82-934b-4a8d-a4a9-394e79961ea4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [126/126 1:13:01, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.315000</td>\n",
              "      <td>0.697662</td>\n",
              "      <td>0.714286</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_state()\n",
        "\n",
        "metrics = trainer.evaluate(prepared_ds['validation'])\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "Pgi9uOUgirUN",
        "outputId": "a74046db-859f-404c-d1f6-eb34b9a9b773"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =         3.0\n",
            "  total_flos               = 145494799GF\n",
            "  train_loss               =      0.4744\n",
            "  train_runtime            =  1:13:45.28\n",
            "  train_samples_per_second =       0.456\n",
            "  train_steps_per_second   =       0.028\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21/21 01:49]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.7143\n",
            "  eval_loss               =     0.6977\n",
            "  eval_runtime            = 0:01:54.44\n",
            "  eval_samples_per_second =      1.468\n",
            "  eval_steps_per_second   =      0.183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# copy results to drive to save them\n",
        "# !cp -r ./vit-base-elephants-v5/ ./drive/MyDrive/ViT"
      ],
      "metadata": {
        "id": "_1XxvuT-8ahy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer.predict(prepared_ds[\"test\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "CX-ePL8s-zXG",
        "outputId": "05b572dc-b7ea-4583-944b-54f2f1aafd8a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions\n"
      ],
      "metadata": {
        "id": "EmUH4jXA_zdt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}